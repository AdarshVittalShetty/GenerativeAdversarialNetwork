{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras import backend\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n",
    "from matplotlib import pyplot\n",
    " \n",
    "    \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D,Lambda,BatchNormalization,Activation\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import load_model\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import Input,MaxPool1D\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences in implementation for the WGAN are as follows:\n",
    "\n",
    "Use a linear activation function in the output layer of the critic model (instead of sigmoid).\n",
    "Use -1 labels for real data and 1 labels for fake data (instead of 1 and 0).\n",
    "Use Wasserstein loss to train the critic and generator models.\n",
    "Constrain critic model weights to a limited range after each mini batch update (e.g. [-0.01,0.01]).\n",
    "Update the critic model more times than the generator each iteration (e.g. 5).\n",
    "Use the RMSProp version of gradient descent with a small learning rate and no momentum (e.g. 0.00005)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max_len = 300 #512\n",
    "b_size = 128\n",
    "n_chan = 10\n",
    "f_size = 3 # filter size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./../deceptive-opinion.csv\")\n",
    "data = data.loc[:,['text','deceptive']]\n",
    "\n",
    "# stem messages\n",
    "#messages = [preprocess(message, stem=True) for message in data.text]\n",
    "data.text = data.text.apply(lambda message : preprocess(message, stem=False))\n",
    "data['deceptive'] = data.deceptive.map({'truthful':-1, 'deceptive':1})\n",
    "\n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "embeddings = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "for doc in data['text'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
    "    temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "    for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
    "        if word not in stopwords: # if word is not present in stopwords then (try)\n",
    "            try:\n",
    "                word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "    doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "docs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors['deceptive'] = data['deceptive']\n",
    "docs_vectors = docs_vectors.dropna()\n",
    "\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(docs_vectors, test_size = 0.2,\n",
    "                                                   random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  649\n",
      "Test set size :  320\n"
     ]
    }
   ],
   "source": [
    "df_truthful = df_train.loc[df_train.deceptive == -1,:]\n",
    "X_train = df_truthful.drop('deceptive', axis = 1)#df_truthful.text\n",
    "X_test=df_test.drop('deceptive', axis = 1)#df_test.text\n",
    "y_train= df_truthful.deceptive\n",
    "y_test=df_test.deceptive\n",
    "\n",
    "print('Training set size : ', (X_train.shape[0]))\n",
    "print('Test set size : ', (X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (649, 300)\n",
      "x_test shape: (320, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = X_train.values#sequence.pad_sequences(X_train_sparse.toarray(), maxlen=max_len)\n",
    "x_test = X_test.values#sequence.pad_sequences(X_test_sparse.toarray(), maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)\n",
    " \n",
    "# define the standalone critic model\n",
    "def define_critic(in_shape=(seq_len,1)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # weight constraint\n",
    "    const = ClipConstraint(0.01)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(n_chan,f_size,activation='relu', padding='same',kernel_initializer=init, kernel_constraint=const,input_shape = (seq_len,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv1D(n_chan,f_size,activation='relu',padding='same', kernel_initializer=init, kernel_constraint=const))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # scoring, linear activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    # compile model\n",
    "    opt = RMSprop(lr=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt,  metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    def Conv1DTranspose(inp,nf,ks,s=2,p='same'):\n",
    "        x1 = Lambda(lambda x : K.expand_dims(x,axis=2))(inp)\n",
    "        x2 = Conv2DTranspose(filters=nf,kernel_size=(ks,1),strides=(s,1),padding=p)(x1)\n",
    "        return Lambda(lambda x :K.squeeze(x,axis=2))(x2)\n",
    "\n",
    "\n",
    "    \n",
    "    G = Sequential()\n",
    "    G.add(Dense(int(seq_len*0.25)*n_chan,input_shape=(latent_dim,)))\n",
    "    G.add(Reshape((int(seq_len*0.25),n_chan)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,n_chan,f_size)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,1,3)))\n",
    "    #G.add(Activation('sigmoid'))\n",
    "    G.add(Activation('tanh'))\n",
    "    \n",
    "    G.summary()\n",
    "    \n",
    "    \n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the combined generator and critic model, for updating the generator\n",
    "def define_gan(generator, critic):\n",
    "    # make weights in the critic not trainable\n",
    "    for layer in critic.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the critic\n",
    "    model.add(critic)\n",
    "    # compile model\n",
    "    opt = RMSprop(lr=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    return model\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    (trainX, _), (_, _) = (x_train,y_train),(x_test,y_test)#load_data()\n",
    "    return trainX \n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select data\n",
    "    X = dataset[ix]\n",
    "    # generate class labels, -1 for 'real'\n",
    "    y = -ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels with 1.0 for 'fake'\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    # plot history\n",
    "    pyplot.title(\"WGAN+WORD2VEC\")\n",
    "    pyplot.plot(d1_hist, label='crit_real')\n",
    "    pyplot.plot(d2_hist, label='crit_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('WGAN_WORD2VEC_line_plot_loss.png')\n",
    "    pyplot.close()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 750)               38250     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 75, 10)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 75, 10)            40        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 150, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 150, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 300, 1)            0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300, 1)            0         \n",
      "=================================================================\n",
      "Total params: 38,330\n",
      "Trainable params: 38,290\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n",
      "(649, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, c1=-0.016, c2=-0.018 g=0.026\n",
      ">2, c1=-0.031, c2=-0.110 g=0.074\n",
      ">3, c1=-0.041, c2=-0.165 g=0.120\n",
      ">4, c1=-0.051, c2=-0.197 g=0.160\n",
      ">5, c1=-0.055, c2=-0.235 g=0.245\n",
      ">6, c1=-0.063, c2=-0.317 g=0.280\n",
      ">7, c1=-0.068, c2=-0.364 g=0.325\n",
      ">8, c1=-0.071, c2=-0.451 g=0.361\n",
      ">9, c1=-0.076, c2=-0.529 g=0.469\n",
      ">10, c1=-0.079, c2=-0.594 g=0.499\n",
      ">11, c1=-0.082, c2=-0.706 g=0.593\n",
      ">12, c1=-0.083, c2=-0.766 g=0.670\n",
      ">13, c1=-0.085, c2=-0.888 g=0.701\n",
      ">14, c1=-0.084, c2=-1.006 g=0.882\n",
      ">15, c1=-0.082, c2=-1.099 g=1.021\n",
      ">16, c1=-0.083, c2=-1.208 g=1.090\n",
      ">17, c1=-0.084, c2=-1.344 g=1.188\n",
      ">18, c1=-0.080, c2=-1.518 g=1.350\n",
      ">19, c1=-0.081, c2=-1.660 g=1.469\n",
      ">20, c1=-0.079, c2=-1.756 g=1.586\n",
      ">21, c1=-0.074, c2=-1.873 g=1.681\n",
      ">22, c1=-0.072, c2=-2.061 g=1.861\n",
      ">23, c1=-0.067, c2=-2.164 g=2.119\n",
      ">24, c1=-0.065, c2=-2.351 g=2.196\n",
      ">25, c1=-0.061, c2=-2.555 g=2.272\n",
      ">26, c1=-0.055, c2=-2.694 g=2.513\n",
      ">27, c1=-0.049, c2=-2.918 g=2.638\n",
      ">28, c1=-0.039, c2=-2.998 g=2.824\n",
      ">29, c1=-0.041, c2=-3.213 g=2.988\n",
      ">30, c1=-0.034, c2=-3.332 g=3.166\n",
      ">31, c1=-0.028, c2=-3.501 g=3.286\n",
      ">32, c1=-0.014, c2=-3.814 g=3.619\n",
      ">33, c1=-0.012, c2=-3.928 g=3.713\n",
      ">34, c1=-0.007, c2=-4.086 g=3.977\n",
      ">35, c1=0.006, c2=-4.321 g=4.098\n",
      ">36, c1=0.008, c2=-4.534 g=4.092\n",
      ">37, c1=0.016, c2=-4.640 g=4.555\n",
      ">38, c1=0.035, c2=-4.873 g=4.702\n",
      ">39, c1=0.039, c2=-5.097 g=4.938\n",
      ">40, c1=0.048, c2=-5.312 g=5.095\n",
      ">41, c1=0.057, c2=-5.569 g=5.288\n",
      ">42, c1=0.061, c2=-5.829 g=5.511\n",
      ">43, c1=0.078, c2=-6.064 g=5.772\n",
      ">44, c1=0.088, c2=-6.182 g=5.964\n",
      ">45, c1=0.094, c2=-6.398 g=6.163\n",
      ">46, c1=0.104, c2=-6.677 g=6.323\n",
      ">47, c1=0.112, c2=-6.820 g=6.555\n",
      ">48, c1=0.131, c2=-7.160 g=6.743\n",
      ">49, c1=0.142, c2=-7.350 g=6.921\n",
      ">50, c1=0.150, c2=-7.593 g=7.263\n",
      ">51, c1=0.170, c2=-7.747 g=7.390\n",
      ">52, c1=0.178, c2=-7.901 g=7.676\n",
      ">53, c1=0.194, c2=-8.210 g=7.809\n",
      ">54, c1=0.196, c2=-8.348 g=8.009\n",
      ">55, c1=0.214, c2=-8.759 g=8.478\n",
      ">56, c1=0.229, c2=-8.919 g=8.550\n",
      ">57, c1=0.240, c2=-9.093 g=8.931\n",
      ">58, c1=0.252, c2=-9.347 g=8.999\n",
      ">59, c1=0.269, c2=-9.429 g=9.081\n",
      ">60, c1=0.274, c2=-9.882 g=9.548\n",
      ">61, c1=0.288, c2=-10.105 g=9.616\n",
      ">62, c1=0.316, c2=-10.172 g=9.804\n",
      ">63, c1=0.308, c2=-10.368 g=9.859\n",
      ">64, c1=0.336, c2=-10.909 g=10.368\n",
      ">65, c1=0.342, c2=-10.987 g=10.635\n",
      ">66, c1=0.356, c2=-11.131 g=10.750\n",
      ">67, c1=0.365, c2=-11.449 g=10.934\n",
      ">68, c1=0.382, c2=-11.594 g=11.085\n",
      ">69, c1=0.402, c2=-11.786 g=11.277\n",
      ">70, c1=0.407, c2=-12.058 g=11.451\n",
      ">71, c1=0.431, c2=-12.353 g=11.840\n",
      ">72, c1=0.440, c2=-12.490 g=12.029\n",
      ">73, c1=0.443, c2=-12.774 g=12.005\n",
      ">74, c1=0.468, c2=-12.908 g=12.321\n",
      ">75, c1=0.482, c2=-13.210 g=12.667\n",
      ">76, c1=0.494, c2=-13.418 g=12.767\n",
      ">77, c1=0.510, c2=-13.554 g=13.207\n",
      ">78, c1=0.520, c2=-13.952 g=13.191\n",
      ">79, c1=0.540, c2=-14.123 g=13.449\n",
      ">80, c1=0.553, c2=-14.268 g=13.593\n",
      ">81, c1=0.557, c2=-14.411 g=13.679\n",
      ">82, c1=0.582, c2=-14.815 g=13.905\n",
      ">83, c1=0.588, c2=-14.937 g=14.119\n",
      ">84, c1=0.606, c2=-15.047 g=14.504\n",
      ">85, c1=0.621, c2=-15.358 g=14.494\n",
      ">86, c1=0.628, c2=-15.614 g=14.600\n",
      ">87, c1=0.641, c2=-15.831 g=15.232\n",
      ">88, c1=0.662, c2=-16.073 g=15.310\n",
      ">89, c1=0.658, c2=-16.270 g=15.578\n",
      ">90, c1=0.672, c2=-16.400 g=15.739\n",
      ">91, c1=0.683, c2=-16.736 g=15.897\n",
      ">92, c1=0.720, c2=-17.047 g=16.152\n",
      ">93, c1=0.718, c2=-17.164 g=16.362\n",
      ">94, c1=0.739, c2=-17.414 g=16.487\n",
      ">95, c1=0.741, c2=-17.435 g=16.649\n",
      ">96, c1=0.754, c2=-17.787 g=16.871\n",
      ">97, c1=0.773, c2=-17.657 g=17.085\n",
      ">98, c1=0.770, c2=-18.261 g=17.185\n",
      ">99, c1=0.798, c2=-18.511 g=17.608\n",
      ">100, c1=0.830, c2=-18.645 g=17.600\n",
      ">101, c1=0.823, c2=-18.946 g=17.972\n",
      ">102, c1=0.835, c2=-19.133 g=17.960\n",
      ">103, c1=0.842, c2=-19.269 g=18.192\n",
      ">104, c1=0.859, c2=-19.609 g=18.467\n",
      ">105, c1=0.874, c2=-19.837 g=18.811\n",
      ">106, c1=0.889, c2=-19.871 g=18.711\n",
      ">107, c1=0.905, c2=-20.368 g=19.023\n",
      ">108, c1=0.918, c2=-20.399 g=19.112\n",
      ">109, c1=0.929, c2=-20.565 g=19.442\n",
      ">110, c1=0.949, c2=-20.800 g=19.695\n",
      ">111, c1=0.954, c2=-20.974 g=19.697\n",
      ">112, c1=0.960, c2=-21.166 g=19.976\n",
      ">113, c1=0.966, c2=-21.399 g=20.522\n",
      ">114, c1=1.001, c2=-21.672 g=20.256\n",
      ">115, c1=1.004, c2=-21.927 g=20.389\n",
      ">116, c1=1.005, c2=-22.108 g=20.969\n",
      ">117, c1=1.019, c2=-22.346 g=21.025\n",
      ">118, c1=1.072, c2=-22.620 g=21.207\n",
      ">119, c1=1.058, c2=-22.792 g=21.396\n",
      ">120, c1=1.088, c2=-23.004 g=21.804\n",
      ">121, c1=1.077, c2=-23.241 g=21.748\n",
      ">122, c1=1.082, c2=-23.413 g=22.258\n",
      ">123, c1=1.125, c2=-23.548 g=21.991\n",
      ">124, c1=1.110, c2=-23.859 g=22.443\n",
      ">125, c1=1.121, c2=-24.072 g=22.657\n",
      ">126, c1=1.159, c2=-24.489 g=22.625\n",
      ">127, c1=1.154, c2=-24.531 g=22.878\n",
      ">128, c1=1.158, c2=-24.640 g=23.201\n",
      ">129, c1=1.185, c2=-24.825 g=23.395\n",
      ">130, c1=1.202, c2=-25.093 g=23.637\n",
      ">131, c1=1.217, c2=-25.518 g=24.010\n",
      ">132, c1=1.223, c2=-25.680 g=24.001\n",
      ">133, c1=1.223, c2=-25.948 g=24.088\n",
      ">134, c1=1.243, c2=-26.044 g=24.582\n",
      ">135, c1=1.261, c2=-26.171 g=24.629\n",
      ">136, c1=1.262, c2=-26.352 g=25.009\n",
      ">137, c1=1.264, c2=-26.577 g=25.137\n",
      ">138, c1=1.294, c2=-26.776 g=25.446\n",
      ">139, c1=1.312, c2=-27.096 g=25.443\n",
      ">140, c1=1.324, c2=-27.460 g=25.325\n",
      ">141, c1=1.321, c2=-27.481 g=25.746\n",
      ">142, c1=1.377, c2=-27.573 g=26.112\n",
      ">143, c1=1.369, c2=-27.810 g=25.993\n",
      ">144, c1=1.379, c2=-28.200 g=26.492\n",
      ">145, c1=1.377, c2=-28.341 g=26.481\n",
      ">146, c1=1.396, c2=-28.641 g=26.855\n",
      ">147, c1=1.418, c2=-28.786 g=26.605\n",
      ">148, c1=1.422, c2=-29.119 g=27.284\n",
      ">149, c1=1.455, c2=-29.186 g=27.429\n",
      ">150, c1=1.454, c2=-29.351 g=27.677\n",
      ">151, c1=1.476, c2=-29.705 g=27.840\n",
      ">152, c1=1.495, c2=-30.078 g=27.759\n",
      ">153, c1=1.500, c2=-30.172 g=27.977\n",
      ">154, c1=1.480, c2=-30.433 g=28.254\n",
      ">155, c1=1.521, c2=-30.781 g=28.520\n",
      ">156, c1=1.528, c2=-30.688 g=28.819\n",
      ">157, c1=1.557, c2=-31.020 g=28.819\n",
      ">158, c1=1.567, c2=-31.398 g=29.069\n",
      ">159, c1=1.564, c2=-31.578 g=29.187\n",
      ">160, c1=1.611, c2=-31.787 g=29.502\n",
      ">161, c1=1.607, c2=-31.576 g=29.686\n",
      ">162, c1=1.633, c2=-32.024 g=29.934\n",
      ">163, c1=1.612, c2=-32.401 g=30.085\n",
      ">164, c1=1.648, c2=-32.521 g=30.394\n",
      ">165, c1=1.639, c2=-33.014 g=30.699\n",
      ">166, c1=1.675, c2=-32.830 g=30.711\n",
      ">167, c1=1.671, c2=-33.407 g=31.143\n",
      ">168, c1=1.695, c2=-33.420 g=30.953\n",
      ">169, c1=1.720, c2=-33.521 g=31.220\n",
      ">170, c1=1.732, c2=-34.120 g=31.150\n",
      ">171, c1=1.756, c2=-34.154 g=31.838\n",
      ">172, c1=1.723, c2=-34.372 g=31.948\n",
      ">173, c1=1.761, c2=-34.401 g=31.939\n",
      ">174, c1=1.759, c2=-34.628 g=32.272\n",
      ">175, c1=1.806, c2=-35.015 g=32.111\n",
      ">176, c1=1.801, c2=-35.264 g=32.342\n",
      ">177, c1=1.778, c2=-35.431 g=32.758\n",
      ">178, c1=1.827, c2=-35.752 g=33.132\n",
      ">179, c1=1.795, c2=-36.042 g=33.504\n",
      ">180, c1=1.843, c2=-36.101 g=33.166\n",
      ">181, c1=1.836, c2=-35.920 g=33.675\n",
      ">182, c1=1.829, c2=-36.364 g=33.596\n",
      ">183, c1=1.883, c2=-36.478 g=33.990\n",
      ">184, c1=1.902, c2=-36.524 g=34.470\n",
      ">185, c1=1.882, c2=-37.040 g=34.452\n",
      ">186, c1=1.902, c2=-37.417 g=34.602\n",
      ">187, c1=1.896, c2=-37.646 g=34.535\n",
      ">188, c1=1.916, c2=-37.694 g=35.205\n",
      ">189, c1=1.952, c2=-37.881 g=35.413\n",
      ">190, c1=1.957, c2=-38.069 g=34.955\n",
      ">191, c1=1.970, c2=-38.536 g=35.831\n",
      ">192, c1=1.981, c2=-38.614 g=35.402\n",
      ">193, c1=2.013, c2=-38.970 g=35.868\n",
      ">194, c1=2.008, c2=-38.806 g=36.113\n",
      ">195, c1=2.029, c2=-39.325 g=36.361\n",
      ">196, c1=2.007, c2=-39.419 g=36.538\n",
      ">197, c1=2.039, c2=-39.336 g=36.513\n",
      ">198, c1=2.046, c2=-40.208 g=36.473\n",
      ">199, c1=2.094, c2=-39.872 g=36.895\n",
      ">200, c1=2.088, c2=-40.214 g=37.352\n"
     ]
    }
   ],
   "source": [
    "# train the generator and critic\n",
    "def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=20, n_batch=64, n_critic=5):\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # lists for keeping track of loss\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # update the critic more than the generator\n",
    "        c1_tmp, c2_tmp = list(), list()\n",
    "        for _ in range(n_critic):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\n",
    "            # update critic model weights\n",
    "            c_loss1 = c_model.train_on_batch(X_real.reshape(X_real.shape[0],X_real.shape[1],1), y_real)\n",
    "            c1_tmp.append(c_loss1[0])\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\n",
    "            \n",
    "            # update critic model weights\n",
    "            c_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
    "            c2_tmp.append(c_loss2[0])\n",
    "        # store critic loss\n",
    "        c1_hist.append(mean(c1_tmp))\n",
    "        c2_hist.append(mean(c2_tmp))\n",
    "        # prepare points in latent space as input for the generator\n",
    "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = -ones((n_batch, 1))\n",
    "        # update the generator via the critic's error\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        g_hist.append(g_loss)\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "\n",
    "    # line plots of loss\n",
    "    plot_history(c1_hist, c2_hist, g_hist)\n",
    "    \n",
    "    return c_model\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 50\n",
    "# create the critic\n",
    "critic = define_critic()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, critic)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)\n",
    "# train model\n",
    "c_model = train(generator, critic, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the loss of the critic and generator models is reported to the console each iteration of the training loop.\n",
    "# Specifically, c1 is the loss of the critic on real examples, c2 is the loss of the critic in generated samples,\n",
    "# and g is the loss of the generator trained via the critic.\n",
    "\n",
    "# The c1 scores are inverted as part of the loss function; this means if they are reported as negative,\n",
    "# then they are really positive, and if they are reported as positive, they are really negative.\n",
    "# The sign of the c2 scores is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
