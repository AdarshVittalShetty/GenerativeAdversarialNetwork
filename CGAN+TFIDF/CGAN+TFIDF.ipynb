{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "Using TensorFlow backend.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D,Lambda,BatchNormalization,Activation\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import multiply\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = max_len = 1024\n",
    "b_size = 128\n",
    "n_chan = 10\n",
    "f_size = 3 # filter size\n",
    "img_shape = (1024, 1)\n",
    "z_dim = 1024\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  649\n",
      "Test set size :  320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"./../deceptive-opinion.csv\")\n",
    "data = data.loc[:,['text','deceptive']]\n",
    "\n",
    "# stem messages\n",
    "#messages = [preprocess(message, stem=True) for message in data.text]\n",
    "data.text = data.text.apply(lambda message : preprocess(message, stem=False))\n",
    "data['deceptive'] = data.deceptive.map({'truthful':1, 'deceptive':0})\n",
    "\n",
    "X = data.text\n",
    "y = data.deceptive\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size = 0.20,random_state=1)\n",
    "\n",
    "\n",
    "df_train, df_test= train_test_split(data,  test_size = 0.20,random_state=1)\n",
    "df_truthful = df_train.loc[df_train.deceptive == 1,:]\n",
    "X_train = df_truthful.text\n",
    "X_test=df_test.text\n",
    "y_train= df_truthful.deceptive\n",
    "y_test=df_test.deceptive\n",
    "\n",
    "print('Training set size : ', (X_train.shape[0]))\n",
    "print('Test set size : ', (X_test.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_model.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((649, 5395), (320, 5395))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape,X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = X_train_tfidf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (649, 1024)\n",
      "x_test shape: (320, 1024)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(X_train_tfidf.toarray(), maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(X_test_tfidf.toarray(), maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "\n",
    "    return (x_train,y_train),(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(max_len,1), n_classes=2):\n",
    "# in_shape=(max_len,1) \n",
    "# n_classes=2\n",
    "    img = Input(shape=in_shape)\n",
    "\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    # embedding layer:\n",
    "    # turns labels into dense vectors \n",
    "    # produces 3D tensor \n",
    "    label_embedding = Embedding(input_dim=n_classes, output_dim=np.prod(in_shape), input_length=1)(label)\n",
    "    # Flatten the embedding 3D tensor into 2D  tensor \n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    # Reshape label embeddings to have same dimensions as input data\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "\n",
    "    # concatenate data with corresponding label embeddings\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    print(concatenated.shape)\n",
    "\n",
    "\n",
    "    D = Sequential()\n",
    "    print(concatenated.shape)\n",
    "\n",
    "    D.add(Conv1D(n_chan,f_size,activation='relu',input_shape = (seq_len,2)))\n",
    "    D.add(Flatten())\n",
    "    D.add(Dense(1, activation='sigmoid'))\n",
    "    D.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    D.summary()\n",
    "\n",
    "    model = D(concatenated)\n",
    "    return Model([img, label], model)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=2):\n",
    "\n",
    "    def Conv1DTranspose(inp,nf,ks,s=2,p='same'):\n",
    "        x1 = Lambda(lambda x : K.expand_dims(x,axis=2))(inp)\n",
    "        x2 = Conv2DTranspose(filters=nf,kernel_size=(ks,1),strides=(s,1),padding=p)(x1)\n",
    "        return Lambda(lambda x :K.squeeze(x,axis=2))(x2)\n",
    "    \n",
    "\n",
    "    z_dim = latent_dim\n",
    "    \n",
    "    z = Input(shape=(z_dim, ))\n",
    "    \n",
    "    # Conditioning label\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    # embedding layer:\n",
    "\n",
    "    label_embedding = Embedding(n_classes, z_dim, input_length=1)(label)\n",
    "\n",
    "    \n",
    "    # Flatten the embedding 3D tensor into 2D  tensor\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    \n",
    "    # Element-wise product of the vectors z and the label embeddings\n",
    "    joined_representation = multiply([z, label_embedding])\n",
    "\n",
    "    G = Sequential()\n",
    "    G.add(Dense(int(seq_len/8)*n_chan,input_shape=(latent_dim,)))\n",
    "    G.add(Reshape((int(seq_len/8),n_chan)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "    for i in range(0,2):\n",
    "        G.add(Lambda(lambda x : Conv1DTranspose(x,n_chan,f_size)))\n",
    "        G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,1,3)))\n",
    "    G.add(Activation('sigmoid'))\n",
    "    G.summary()\n",
    "    \n",
    "    model = G(joined_representation)\n",
    "    \n",
    "    return Model([z, label], model)\n",
    "\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1024, 2)\n",
      "(?, 1024, 2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1022, 10)          70        \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10220)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 10221     \n",
      "=================================================================\n",
      "Total params: 10,291\n",
      "Trainable params: 10,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1280)              1312000   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 128, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 256, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 512, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 1024, 1)           0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024, 1)           0         \n",
      "=================================================================\n",
      "Total params: 1,312,120\n",
      "Trainable params: 1,312,060\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = define_discriminator()\n",
    "disc.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
    "\n",
    "# build the generator\n",
    "gen = define_generator(z_dim)\n",
    "\n",
    "# the generator takes noise and the target label as input\n",
    "# and generates the corresponding digit for that label\n",
    "z = Input(shape=(z_dim,))\n",
    "label = Input(shape=(1,))\n",
    "\n",
    "img = gen([z, label])\n",
    "\n",
    "# keep the discriminator's params constant for generator training\n",
    "disc.trainable = False\n",
    "\n",
    "prediction = disc([img, label])\n",
    "\n",
    "# Conditional (Conditional) GAN model with fixed discriminator to train the generator\n",
    "cgan = Model([z, label], prediction)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    # plot history\n",
    "    pyplot.title(\"CGAN+TFIDF\")\n",
    "    pyplot.plot(d1_hist, label='disc_real')\n",
    "    pyplot.plot(d2_hist, label='disc_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('CGAN_TFIDF_line_plot_loss.png')\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "def train(iterations, batch_size, sample_interval):\n",
    "    \n",
    "    (X_train, y_train), (_, _) = load_real_samples()\n",
    "    \n",
    "    \n",
    "    real = np.ones(shape=(batch_size, 1))\n",
    "    fake = np.zeros(shape=(batch_size, 1))\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "\n",
    "        \n",
    "        imgs, labels = X_train[idx], np.array(y_train)[idx]\n",
    "\n",
    "        \n",
    "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "\n",
    "        \n",
    "        gen_imgs = gen.predict([z, labels])\n",
    "        #print(gen_imgs.shape)\n",
    "        \n",
    "        d_loss_real = disc.train_on_batch([imgs.reshape(128,max_len,1), labels], real)\n",
    "        d_loss_fake = disc.train_on_batch([gen_imgs, labels], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "        \n",
    "        g_loss = cgan.train_on_batch([z, labels], real)\n",
    "        \n",
    "        c1_hist.append(d_loss_real[0])\n",
    "        c2_hist.append(d_loss_fake[0])\n",
    "        g_hist.append(g_loss)\n",
    "        \n",
    "        if iteration % sample_interval == 0:\n",
    "            print('{} [D loss: {}, accuracy: {:.2f}] [D1 loss: {}][D2 loss: {}][G loss: {}]'.format(iteration, d_loss[0], 100 * d_loss[1],d_loss_real,d_loss_fake, g_loss))\n",
    "        \n",
    "            losses.append((d_loss[0], g_loss))\n",
    "            accuracies.append(d_loss[1])\n",
    "\n",
    "            \n",
    "    plot_history(c1_hist, c2_hist, g_hist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.7111334800720215, accuracy: 50.00] [D1 loss: [0.6902804, 1.0]][D2 loss: [0.73198664, 0.0]][G loss: 1.1012836694717407]\n",
      "1 [D loss: 0.4524542987346649, accuracy: 100.00] [D1 loss: [0.6648891, 1.0]][D2 loss: [0.24001949, 1.0]][G loss: 2.137694835662842]\n",
      "2 [D loss: 0.35572874546051025, accuracy: 100.00] [D1 loss: [0.6458642, 1.0]][D2 loss: [0.06559327, 1.0]][G loss: 3.2337448596954346]\n",
      "3 [D loss: 0.32105177640914917, accuracy: 100.00] [D1 loss: [0.6212401, 1.0]][D2 loss: [0.020863503, 1.0]][G loss: 4.214055061340332]\n",
      "4 [D loss: 0.29831385612487793, accuracy: 100.00] [D1 loss: [0.5884163, 1.0]][D2 loss: [0.008211412, 1.0]][G loss: 4.977941989898682]\n",
      "5 [D loss: 0.27606743574142456, accuracy: 100.00] [D1 loss: [0.5483346, 1.0]][D2 loss: [0.0038002604, 1.0]][G loss: 5.537710189819336]\n",
      "6 [D loss: 0.2523312270641327, accuracy: 100.00] [D1 loss: [0.5025724, 1.0]][D2 loss: [0.0020900161, 1.0]][G loss: 6.020606994628906]\n",
      "7 [D loss: 0.22694998979568481, accuracy: 100.00] [D1 loss: [0.4526176, 1.0]][D2 loss: [0.0012824079, 1.0]][G loss: 6.36142635345459]\n",
      "8 [D loss: 0.20045121014118195, accuracy: 100.00] [D1 loss: [0.40001398, 1.0]][D2 loss: [0.0008884235, 1.0]][G loss: 6.639507293701172]\n",
      "9 [D loss: 0.1735386997461319, accuracy: 100.00] [D1 loss: [0.346438, 1.0]][D2 loss: [0.00063941267, 1.0]][G loss: 6.84130859375]\n",
      "10 [D loss: 0.14702798426151276, accuracy: 100.00] [D1 loss: [0.29355237, 1.0]][D2 loss: [0.000503602, 1.0]][G loss: 6.935103416442871]\n",
      "11 [D loss: 0.12175942957401276, accuracy: 100.00] [D1 loss: [0.24310425, 1.0]][D2 loss: [0.000414603, 1.0]][G loss: 7.03280782699585]\n",
      "12 [D loss: 0.09857912361621857, accuracy: 100.00] [D1 loss: [0.19679455, 1.0]][D2 loss: [0.0003636862, 1.0]][G loss: 7.121625900268555]\n",
      "13 [D loss: 0.07814506441354752, accuracy: 100.00] [D1 loss: [0.15597613, 1.0]][D2 loss: [0.0003139968, 1.0]][G loss: 7.120011329650879]\n",
      "14 [D loss: 0.060816165059804916, accuracy: 100.00] [D1 loss: [0.12134005, 1.0]][D2 loss: [0.00029227594, 1.0]][G loss: 7.0980119705200195]\n",
      "15 [D loss: 0.046664997935295105, accuracy: 100.00] [D1 loss: [0.09305483, 1.0]][D2 loss: [0.0002751639, 1.0]][G loss: 7.124781131744385]\n",
      "16 [D loss: 0.035487305372953415, accuracy: 100.00] [D1 loss: [0.07071703, 1.0]][D2 loss: [0.0002575802, 1.0]][G loss: 7.091863632202148]\n",
      "17 [D loss: 0.02690737694501877, accuracy: 100.00] [D1 loss: [0.0535633, 1.0]][D2 loss: [0.0002514539, 1.0]][G loss: 7.052900791168213]\n",
      "18 [D loss: 0.0204515028744936, accuracy: 100.00] [D1 loss: [0.040659543, 1.0]][D2 loss: [0.00024346214, 1.0]][G loss: 7.01341438293457]\n",
      "19 [D loss: 0.01566271297633648, accuracy: 100.00] [D1 loss: [0.031085711, 1.0]][D2 loss: [0.00023971422, 1.0]][G loss: 6.95692777633667]\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "batch_size = 128\n",
    "sample_interval = 1#1000\n",
    "\n",
    "train(iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
