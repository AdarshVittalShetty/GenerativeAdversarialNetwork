{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "Using TensorFlow backend.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D,Lambda,BatchNormalization,Activation\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import multiply\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max_len = 300 #512\n",
    "b_size = 128\n",
    "n_chan = 10\n",
    "f_size = 3 # filter size\n",
    "\n",
    "img_shape = (300, 1)\n",
    "z_dim = 300\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../deceptive-opinion.csv\")\n",
    "data = data.loc[:,['text','deceptive']]\n",
    "\n",
    "# stem messages\n",
    "#messages = [preprocess(message, stem=True) for message in data.text]\n",
    "data.text = data.text.apply(lambda message : preprocess(message, stem=False))\n",
    "data['deceptive'] = data.deceptive.map({'truthful':1, 'deceptive':0})\n",
    "\n",
    "\n",
    "\n",
    "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "embeddings = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "for doc in data['text'].str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
    "    temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "    for word in doc.split(' '): # looping through each word of a single document and spliting through space\n",
    "        if word not in stopwords: # if word is not present in stopwords then (try)\n",
    "            try:\n",
    "                word_vec = embeddings[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "    doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "docs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors['deceptive'] = data['deceptive']\n",
    "docs_vectors = docs_vectors.dropna()\n",
    "\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(docs_vectors, test_size = 0.2,\n",
    "                                                   random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truthful = df_train.loc[df_train.deceptive == 1,:]\n",
    "X_train = df_truthful.drop('deceptive', axis = 1)#df_truthful.text\n",
    "X_test=df_test.drop('deceptive', axis = 1)#df_test.text\n",
    "y_train= df_truthful.deceptive\n",
    "y_test=df_test.deceptive\n",
    "\n",
    "print('Training set size : ', (X_train.shape[0]))\n",
    "print('Test set size : ', (X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = X_train.values#sequence.pad_sequences(X_train_sparse.toarray(), maxlen=max_len)\n",
    "x_test = X_test.values#sequence.pad_sequences(X_test_sparse.toarray(), maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_real_samples():\n",
    "    return (x_train,y_train),(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(max_len,1), n_classes=2):\n",
    "# in_shape=(max_len,1) \n",
    "# n_classes=2\n",
    "    img = Input(shape=in_shape)\n",
    "\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    # embedding layer:\n",
    "    # turns labels into dense vectors \n",
    "    # produces 3D tensor \n",
    "    label_embedding = Embedding(input_dim=n_classes, output_dim=np.prod(in_shape), input_length=1)(label)\n",
    "    # Flatten the embedding 3D tensor into 2D  tensor\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    # Reshape label embeddings to have same dimensions as input data\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "\n",
    "    # concatenate data with corresponding label embeddings\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    print(concatenated.shape)\n",
    "\n",
    "\n",
    "    D = Sequential()\n",
    "    print(concatenated.shape)\n",
    "\n",
    "    D.add(Conv1D(n_chan,f_size,activation='relu',input_shape = (seq_len,2)))\n",
    "    D.add(Flatten())\n",
    "    D.add(Dense(1, activation='sigmoid'))\n",
    "    D.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    D.summary()\n",
    "\n",
    "    model = D(concatenated)\n",
    "    return Model([img, label], model)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=2):\n",
    "\n",
    "    def Conv1DTranspose(inp,nf,ks,s=2,p='same'):\n",
    "        x1 = Lambda(lambda x : K.expand_dims(x,axis=2))(inp)\n",
    "        x2 = Conv2DTranspose(filters=nf,kernel_size=(ks,1),strides=(s,1),padding=p)(x1)\n",
    "        return Lambda(lambda x :K.squeeze(x,axis=2))(x2)\n",
    "    \n",
    "\n",
    "    z_dim = latent_dim\n",
    "    \n",
    "    z = Input(shape=(z_dim, ))\n",
    "    \n",
    "    # Conditioning label\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    # embedding layer:\n",
    "    # turns labels into dense vectors of size z_dim\n",
    "    # produces 3D tensor with shape: (batch_size, 1, z_dim)\n",
    "\n",
    "    label_embedding = Embedding(n_classes, z_dim, input_length=1)(label)\n",
    "    \n",
    "\n",
    "    # Flatten the embedding 3D tensor into 2D  tensor with shape: (batch_size, z_dim)\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    \n",
    "    # Element-wise product of the vectors z and the label embeddings\n",
    "    joined_representation = multiply([z, label_embedding])\n",
    "\n",
    "\n",
    "    \n",
    "    G = Sequential()\n",
    "    G.add(Dense(int(seq_len*0.25)*n_chan,input_shape=(latent_dim,)))\n",
    "    G.add(Reshape((int(seq_len*0.25),n_chan)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,n_chan,f_size)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,1,3)))\n",
    "    G.add(Activation('sigmoid'))\n",
    "    G.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = G(joined_representation)\n",
    "    \n",
    "    return Model([z, label], model)\n",
    "\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = define_discriminator()\n",
    "disc.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
    "\n",
    "# build the generator\n",
    "gen = define_generator(z_dim)\n",
    "\n",
    "# the generator takes noise and the target label as input\n",
    "# and generates the corresponding digit for that label\n",
    "z = Input(shape=(z_dim,))\n",
    "label = Input(shape=(1,))\n",
    "\n",
    "img = gen([z, label])\n",
    "\n",
    "# keep the discriminator's params constant for generator training\n",
    "disc.trainable = False\n",
    "\n",
    "prediction = disc([img, label])\n",
    "\n",
    "# Conditional (Conditional) GAN model with fixed discriminator to train the generator\n",
    "cgan = Model([z, label], prediction)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    # plot history\n",
    "    pyplot.title(\"CGAN+WORD2VEC\")\n",
    "    pyplot.plot(d1_hist, label='disc_real')\n",
    "    pyplot.plot(d2_hist, label='disc_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('CGAN_WORD2VEC_line_plot_loss.png')\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "def train(iterations, batch_size, sample_interval):\n",
    "    \n",
    "    (X_train, y_train), (_, _) = load_real_samples()\n",
    "\n",
    "    \n",
    "    real = np.ones(shape=(batch_size, 1))\n",
    "    fake = np.zeros(shape=(batch_size, 1))\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "\n",
    "        \n",
    "        imgs, labels = X_train[idx], np.array(y_train)[idx]\n",
    "\n",
    "        \n",
    "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "\n",
    "        \n",
    "        gen_imgs = gen.predict([z, labels])\n",
    "\n",
    "        \n",
    "        d_loss_real = disc.train_on_batch([imgs.reshape(128,max_len,1), labels], real)\n",
    "        d_loss_fake = disc.train_on_batch([gen_imgs, labels], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "        \n",
    "        g_loss = cgan.train_on_batch([z, labels], real)\n",
    "        \n",
    "        c1_hist.append(d_loss_real[0])\n",
    "        c2_hist.append(d_loss_fake[0])\n",
    "        g_hist.append(g_loss)\n",
    "        \n",
    "        if iteration % sample_interval == 0:\n",
    "            print('{} [D loss: {}, accuracy: {:.2f}] [D1 loss: {}][D2 loss: {}][G loss: {}]'.format(iteration, d_loss[0], 100 * d_loss[1],d_loss_real,d_loss_fake, g_loss))\n",
    "        \n",
    "            losses.append((d_loss[0], g_loss))\n",
    "            accuracies.append(d_loss[1])\n",
    "\n",
    "            \n",
    "    plot_history(c1_hist, c2_hist, g_hist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 20\n",
    "batch_size = 128\n",
    "sample_interval = 1#1000\n",
    "\n",
    "train(iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
