{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D,Lambda,BatchNormalization,Activation\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import load_model\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import Input,MaxPool1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max_len = 1024 #512\n",
    "b_size = 128\n",
    "n_chan = 10\n",
    "f_size = 3 # filter size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  649\n",
      "Test set size :  320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"./../deceptive-opinion.csv\")\n",
    "data = data.loc[:,['text','deceptive']]\n",
    "\n",
    "# stem messages\n",
    "#messages = [preprocess(message, stem=True) for message in data.text]\n",
    "data.text = data.text.apply(lambda message : preprocess(message, stem=False))\n",
    "data['deceptive'] = data.deceptive.map({'truthful':1, 'deceptive':0})\n",
    "\n",
    "X = data.text\n",
    "y = data.deceptive\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size = 0.30,random_state=1)\n",
    "\n",
    "\n",
    "df_train, df_test= train_test_split(data,  test_size = 0.20,random_state=1)\n",
    "df_truthful = df_train.loc[df_train.deceptive == 1,:]\n",
    "X_train = df_truthful.text\n",
    "X_test=df_test.text\n",
    "y_train= df_truthful.deceptive\n",
    "y_test=df_test.deceptive\n",
    "\n",
    "print('Training set size : ', (X_train.shape[0]))\n",
    "print('Test set size : ', (X_test.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_model.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((649, 5395), (320, 5395))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape,X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = X_train_tfidf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (649, 1024)\n",
      "x_test shape: (320, 1024)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(X_train_tfidf.toarray(), maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(X_test_tfidf.toarray(), maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_discriminator(in_shape=(max_len,1)):\n",
    "\n",
    "    D = Sequential()\n",
    "    D.add(Conv1D(n_chan,f_size,activation='relu',input_shape = (seq_len,1)))\n",
    "    D.add(Flatten())\n",
    "    D.add(Dense(1, activation='sigmoid'))\n",
    "    D.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    return D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    def Conv1DTranspose(inp,nf,ks,s=2,p='same'):\n",
    "        x1 = Lambda(lambda x : K.expand_dims(x,axis=2))(inp)\n",
    "        x2 = Conv2DTranspose(filters=nf,kernel_size=(ks,1),strides=(s,1),padding=p)(x1)\n",
    "        return Lambda(lambda x :K.squeeze(x,axis=2))(x2)\n",
    "\n",
    "    G = Sequential()\n",
    "    G.add(Dense(int(seq_len/8)*n_chan,input_shape=(latent_dim,)))\n",
    "    G.add(Reshape((int(seq_len/8),n_chan)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "    for i in range(0,2):\n",
    "        G.add(Lambda(lambda x : Conv1DTranspose(x,n_chan,f_size)))\n",
    "        G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,1,3)))\n",
    "    G.add(Activation('sigmoid'))\n",
    "    G.summary()\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Dara\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    (trainX, _), (_, _) = (x_train,y_train),(x_test,y_test)#load_data()\n",
    "\n",
    "    return trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select data\n",
    "    X = dataset[ix]\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "    # use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    # plot history\n",
    "    pyplot.title(\"GAN+TFIDF\")\n",
    "    pyplot.plot(d1_hist, label='disc_real')\n",
    "    pyplot.plot(d2_hist, label='disc_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('GAN_TFIDF_line_plot_loss.png')\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=20, n_batch=128):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            \n",
    "       \n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "            (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "            c1_hist.append(d_loss1)\n",
    "            c2_hist.append(d_loss2)\n",
    "            g_hist.append(g_loss)\n",
    "            \n",
    "            #print(X_real.shape,X_fake.shape)\n",
    "            \n",
    "    # save the generator model\n",
    "    #g_model.save('generator.h5')\n",
    "    #d_model.save('dis_generator.h5')\n",
    "    plot_history(c1_hist, c2_hist, g_hist)\n",
    "    return g_model,d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1280)              129280    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 128, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 256, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 512, 10)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512, 10)           40        \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 1024, 1)           0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024, 1)           0         \n",
      "=================================================================\n",
      "Total params: 129,400\n",
      "Trainable params: 129,340\n",
      "Non-trainable params: 60\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/5, d1=0.693, d2=0.622 g=1.107\n",
      ">1, 2/5, d1=0.694, d2=0.268 g=1.905\n",
      ">1, 3/5, d1=0.693, d2=0.097 g=2.832\n",
      ">1, 4/5, d1=0.693, d2=0.037 g=3.704\n",
      ">1, 5/5, d1=0.695, d2=0.015 g=4.471\n",
      ">2, 1/5, d1=0.692, d2=0.007 g=5.124\n",
      ">2, 2/5, d1=0.691, d2=0.004 g=5.674\n",
      ">2, 3/5, d1=0.690, d2=0.002 g=6.129\n",
      ">2, 4/5, d1=0.690, d2=0.002 g=6.528\n",
      ">2, 5/5, d1=0.689, d2=0.001 g=6.852\n",
      ">3, 1/5, d1=0.689, d2=0.001 g=7.123\n",
      ">3, 2/5, d1=0.688, d2=0.001 g=7.356\n",
      ">3, 3/5, d1=0.687, d2=0.000 g=7.551\n",
      ">3, 4/5, d1=0.687, d2=0.000 g=7.703\n",
      ">3, 5/5, d1=0.686, d2=0.000 g=7.851\n",
      ">4, 1/5, d1=0.685, d2=0.000 g=7.961\n",
      ">4, 2/5, d1=0.685, d2=0.000 g=8.056\n",
      ">4, 3/5, d1=0.684, d2=0.000 g=8.124\n",
      ">4, 4/5, d1=0.683, d2=0.000 g=8.199\n",
      ">4, 5/5, d1=0.683, d2=0.000 g=8.249\n",
      ">5, 1/5, d1=0.682, d2=0.000 g=8.302\n",
      ">5, 2/5, d1=0.681, d2=0.000 g=8.324\n",
      ">5, 3/5, d1=0.681, d2=0.000 g=8.363\n",
      ">5, 4/5, d1=0.680, d2=0.000 g=8.376\n",
      ">5, 5/5, d1=0.679, d2=0.000 g=8.402\n",
      ">6, 1/5, d1=0.679, d2=0.000 g=8.423\n",
      ">6, 2/5, d1=0.678, d2=0.000 g=8.433\n",
      ">6, 3/5, d1=0.677, d2=0.000 g=8.447\n",
      ">6, 4/5, d1=0.676, d2=0.000 g=8.444\n",
      ">6, 5/5, d1=0.676, d2=0.000 g=8.453\n",
      ">7, 1/5, d1=0.675, d2=0.000 g=8.460\n",
      ">7, 2/5, d1=0.674, d2=0.000 g=8.472\n",
      ">7, 3/5, d1=0.674, d2=0.000 g=8.454\n",
      ">7, 4/5, d1=0.673, d2=0.000 g=8.461\n",
      ">7, 5/5, d1=0.672, d2=0.000 g=8.485\n",
      ">8, 1/5, d1=0.672, d2=0.000 g=8.481\n",
      ">8, 2/5, d1=0.671, d2=0.000 g=8.485\n",
      ">8, 3/5, d1=0.670, d2=0.000 g=8.482\n",
      ">8, 4/5, d1=0.670, d2=0.000 g=8.482\n",
      ">8, 5/5, d1=0.669, d2=0.000 g=8.487\n",
      ">9, 1/5, d1=0.668, d2=0.000 g=8.484\n",
      ">9, 2/5, d1=0.668, d2=0.000 g=8.479\n",
      ">9, 3/5, d1=0.667, d2=0.000 g=8.470\n",
      ">9, 4/5, d1=0.666, d2=0.000 g=8.489\n",
      ">9, 5/5, d1=0.666, d2=0.000 g=8.473\n",
      ">10, 1/5, d1=0.665, d2=0.000 g=8.476\n",
      ">10, 2/5, d1=0.664, d2=0.000 g=8.491\n",
      ">10, 3/5, d1=0.664, d2=0.000 g=8.479\n",
      ">10, 4/5, d1=0.663, d2=0.000 g=8.490\n",
      ">10, 5/5, d1=0.662, d2=0.000 g=8.480\n",
      ">11, 1/5, d1=0.662, d2=0.000 g=8.477\n",
      ">11, 2/5, d1=0.661, d2=0.000 g=8.473\n",
      ">11, 3/5, d1=0.660, d2=0.000 g=8.489\n",
      ">11, 4/5, d1=0.660, d2=0.000 g=8.483\n",
      ">11, 5/5, d1=0.659, d2=0.000 g=8.483\n",
      ">12, 1/5, d1=0.658, d2=0.000 g=8.480\n",
      ">12, 2/5, d1=0.658, d2=0.000 g=8.489\n",
      ">12, 3/5, d1=0.657, d2=0.000 g=8.470\n",
      ">12, 4/5, d1=0.656, d2=0.000 g=8.477\n",
      ">12, 5/5, d1=0.656, d2=0.000 g=8.493\n",
      ">13, 1/5, d1=0.655, d2=0.000 g=8.483\n",
      ">13, 2/5, d1=0.654, d2=0.000 g=8.491\n",
      ">13, 3/5, d1=0.654, d2=0.000 g=8.489\n",
      ">13, 4/5, d1=0.653, d2=0.000 g=8.480\n",
      ">13, 5/5, d1=0.652, d2=0.000 g=8.473\n",
      ">14, 1/5, d1=0.652, d2=0.000 g=8.477\n",
      ">14, 2/5, d1=0.651, d2=0.000 g=8.482\n",
      ">14, 3/5, d1=0.650, d2=0.000 g=8.489\n",
      ">14, 4/5, d1=0.650, d2=0.000 g=8.488\n",
      ">14, 5/5, d1=0.649, d2=0.000 g=8.499\n",
      ">15, 1/5, d1=0.648, d2=0.000 g=8.495\n",
      ">15, 2/5, d1=0.648, d2=0.000 g=8.491\n",
      ">15, 3/5, d1=0.647, d2=0.000 g=8.493\n",
      ">15, 4/5, d1=0.646, d2=0.000 g=8.479\n",
      ">15, 5/5, d1=0.646, d2=0.000 g=8.500\n",
      ">16, 1/5, d1=0.645, d2=0.000 g=8.489\n",
      ">16, 2/5, d1=0.644, d2=0.000 g=8.490\n",
      ">16, 3/5, d1=0.644, d2=0.000 g=8.500\n",
      ">16, 4/5, d1=0.643, d2=0.000 g=8.493\n",
      ">16, 5/5, d1=0.643, d2=0.000 g=8.495\n",
      ">17, 1/5, d1=0.642, d2=0.000 g=8.496\n",
      ">17, 2/5, d1=0.641, d2=0.000 g=8.483\n",
      ">17, 3/5, d1=0.641, d2=0.000 g=8.476\n",
      ">17, 4/5, d1=0.640, d2=0.000 g=8.483\n",
      ">17, 5/5, d1=0.639, d2=0.000 g=8.485\n",
      ">18, 1/5, d1=0.639, d2=0.000 g=8.498\n",
      ">18, 2/5, d1=0.638, d2=0.000 g=8.493\n",
      ">18, 3/5, d1=0.637, d2=0.000 g=8.486\n",
      ">18, 4/5, d1=0.637, d2=0.000 g=8.505\n",
      ">18, 5/5, d1=0.636, d2=0.000 g=8.496\n",
      ">19, 1/5, d1=0.635, d2=0.000 g=8.503\n",
      ">19, 2/5, d1=0.635, d2=0.000 g=8.511\n",
      ">19, 3/5, d1=0.634, d2=0.000 g=8.480\n",
      ">19, 4/5, d1=0.633, d2=0.000 g=8.509\n",
      ">19, 5/5, d1=0.633, d2=0.000 g=8.485\n",
      ">20, 1/5, d1=0.632, d2=0.000 g=8.498\n",
      ">20, 2/5, d1=0.632, d2=0.000 g=8.503\n",
      ">20, 3/5, d1=0.631, d2=0.000 g=8.498\n",
      ">20, 4/5, d1=0.630, d2=0.000 g=8.498\n",
      ">20, 5/5, d1=0.630, d2=0.000 g=8.498\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# load data\n",
    "dataset = load_real_samples()\n",
    "#print(\"shape -->\",dataset.shape)\n",
    "dataset = np.reshape(dataset,(dataset.shape[0],dataset.shape[1],1))\n",
    "\n",
    "# train model\n",
    "model,d_model= train(generator, discriminator, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
