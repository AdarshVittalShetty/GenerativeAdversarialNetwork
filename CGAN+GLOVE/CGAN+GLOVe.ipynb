{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# load all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D,Lambda,BatchNormalization,Activation\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import multiply\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer and the Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, removes stopwords and lemmatizes/stems the remainder of the sentence'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max_len = 200 #512\n",
    "b_size = 128\n",
    "n_chan = 10\n",
    "f_size = 3 # filter size\n",
    "img_shape = (max_len, 1)\n",
    "z_dim = max_len\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "glove_input_file = './../glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove.6B.200d.w2vformat.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.w2vformat.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec(sent):\n",
    "    wv_res = np.zeros(glove_model.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in glove_model:\n",
    "            ctr += 1\n",
    "            wv_res += glove_model[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    #return (wv_res, ctr)\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pd.read_csv(\"./../deceptive-opinion.csv\")\n",
    "data = data.loc[:,['text','deceptive']]\n",
    "\n",
    "# stem messages\n",
    "#messages = [preprocess(message, stem=True) for message in data.text]\n",
    "data.text = data.text.apply(lambda message : preprocess(message, stem=False))\n",
    "data['deceptive'] = data.deceptive.map({'truthful':1, 'deceptive':0})\n",
    "\n",
    "\n",
    "\n",
    "df_train, df_test= train_test_split(data,  test_size = 0.20,random_state=1)\n",
    "df_truthful = df_train.loc[df_train.deceptive == 1,:]\n",
    "X_train = df_truthful.text\n",
    "X_test=df_test.text\n",
    "y_train= df_truthful.deceptive\n",
    "y_test=df_test.deceptive\n",
    "\n",
    "print('Training set size : ', (X_train.shape[0]))\n",
    "print('Test set size : ', (X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for idx,row in df_truthful.iterrows():\n",
    "    label, text = row['deceptive'],row['text']\n",
    "    X.append(text.split())\n",
    "    y.append(label)\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = [], []\n",
    "for idx,row in df_test.iterrows():\n",
    "    label, text = row['deceptive'],row['text']\n",
    "    X_test.append(text.split())\n",
    "    y_test.append(label)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "train_doc_vecs = pd.DataFrame()\n",
    "for doc in X:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    b = sent_vec(doc_words)\n",
    "    np_array = np.array(b.tolist())\n",
    "    reshaped_array = np.reshape(np_array, (1,200))\n",
    "    a_dataframe = pd.DataFrame(reshaped_array)\n",
    "    train_doc_vecs = train_doc_vecs.append(a_dataframe)\n",
    "\n",
    "    \n",
    "test_doc_vecs = pd.DataFrame()\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    b = sent_vec(doc_words)\n",
    "    np_array = np.array(b.tolist())\n",
    "    reshaped_array = np.reshape(np_array, (1,200))\n",
    "    a_dataframe = pd.DataFrame(reshaped_array)\n",
    "    test_doc_vecs = test_doc_vecs.append(a_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = train_doc_vecs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train =  train_doc_vecs.values#sequence.pad_sequences(train_doc_vecs.toarray(), maxlen=max_len)\n",
    "x_test =  test_doc_vecs.values#sequence.pad_sequences(test_doc_vecs.toarray(), maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_real_samples():\n",
    "    # load dataset\n",
    "    return (x_train,y_train),(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(max_len,1), n_classes=2):\n",
    "# in_shape=(max_len,1) \n",
    "# n_classes=2\n",
    "    img = Input(shape=in_shape)\n",
    "\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    # embedding layer:\n",
    "    # turns labels into dense vectors \n",
    "    # produces 3D tensor\n",
    "    label_embedding = Embedding(input_dim=n_classes, output_dim=np.prod(in_shape), input_length=1)(label)\n",
    "    # Flatten the embedding 3D tensor into 2D  tensor\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    # Reshape label embeddings to have same dimensions as input data\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "\n",
    "    # concatenate data with corresponding label embeddings\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    print(concatenated.shape)\n",
    "\n",
    "\n",
    "    D = Sequential()\n",
    "    print(concatenated.shape)\n",
    "\n",
    "    D.add(Conv1D(n_chan,f_size,activation='relu',input_shape = (seq_len,2)))\n",
    "    D.add(Flatten())\n",
    "    D.add(Dense(1, activation='sigmoid'))\n",
    "    D.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    D.summary()\n",
    "\n",
    "    model = D(concatenated)\n",
    "    return Model([img, label], model)\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=2):\n",
    "\n",
    "    def Conv1DTranspose(inp,nf,ks,s=2,p='same'):\n",
    "        x1 = Lambda(lambda x : K.expand_dims(x,axis=2))(inp)\n",
    "        x2 = Conv2DTranspose(filters=nf,kernel_size=(ks,1),strides=(s,1),padding=p)(x1)\n",
    "        return Lambda(lambda x :K.squeeze(x,axis=2))(x2)\n",
    "    \n",
    "\n",
    "    z_dim = latent_dim\n",
    "    \n",
    "    z = Input(shape=(z_dim, ))\n",
    "    \n",
    "    # Conditioning label\n",
    "    label = Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    # embedding layer:\n",
    "    # turns labels into dense vectors of size z_dim\n",
    "    # produces 3D tensor with shape: (batch_size, 1, z_dim)\n",
    "\n",
    "    label_embedding = Embedding(n_classes, z_dim, input_length=1)(label)\n",
    "\n",
    "    \n",
    "    # Flatten the embedding 3D tensor into 2D  tensor with shape: (batch_size, z_dim)\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    \n",
    "    # Element-wise product of the vectors z and the label embeddings\n",
    "    joined_representation = multiply([z, label_embedding])\n",
    "\n",
    "    G = Sequential()\n",
    "    G.add(Dense(int(seq_len/8)*n_chan,input_shape=(latent_dim,)))\n",
    "    G.add(Reshape((int(seq_len/8),n_chan)))\n",
    "    G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "    for i in range(0,2):\n",
    "        G.add(Lambda(lambda x : Conv1DTranspose(x,n_chan,f_size)))\n",
    "        G.add(BatchNormalization(momentum= 0.8,epsilon=1.e-5))\n",
    "\n",
    "    G.add(Lambda(lambda x : Conv1DTranspose(x,1,3)))\n",
    "    G.add(Activation('sigmoid'))\n",
    "    G.summary()\n",
    "    \n",
    "    model = G(joined_representation)\n",
    "    \n",
    "    return Model([z, label], model)\n",
    "\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = define_discriminator()\n",
    "disc.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
    "\n",
    "# build the generator\n",
    "gen = define_generator(z_dim)\n",
    "\n",
    "# the generator takes noise and the target label as input\n",
    "# and generates the corresponding digit for that label\n",
    "z = Input(shape=(z_dim,))\n",
    "label = Input(shape=(1,))\n",
    "\n",
    "img = gen([z, label])\n",
    "\n",
    "# keep the discriminator's params constant for generator training\n",
    "disc.trainable = False\n",
    "\n",
    "prediction = disc([img, label])\n",
    "\n",
    "# Conditional (Conditional) GAN model with fixed discriminator to train the generator\n",
    "cgan = Model([z, label], prediction)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    # plot history\n",
    "    pyplot.title(\"CGAN+GLOVE\")\n",
    "    pyplot.plot(d1_hist, label='disc_real')\n",
    "    pyplot.plot(d2_hist, label='disc_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('CGAN_GLOVE_line_plot_loss.png')\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "def train(iterations, batch_size, sample_interval):\n",
    "    \n",
    "    (X_train, y_train), (_, _) = load_real_samples()\n",
    "    \n",
    "\n",
    "    \n",
    "    real = np.ones(shape=(batch_size, 1))\n",
    "    fake = np.zeros(shape=(batch_size, 1))\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        \n",
    "        \n",
    "        imgs, labels = X_train[idx], np.array(y_train)[idx]\n",
    "\n",
    "        \n",
    "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "\n",
    "        \n",
    "        gen_imgs = gen.predict([z, labels])\n",
    "\n",
    "        \n",
    "        d_loss_real = disc.train_on_batch([imgs.reshape(128,max_len,1), labels], real)\n",
    "        d_loss_fake = disc.train_on_batch([gen_imgs, labels], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
    "        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
    "        \n",
    "        g_loss = cgan.train_on_batch([z, labels], real)\n",
    "        \n",
    "        c1_hist.append(d_loss_real[0])\n",
    "        c2_hist.append(d_loss_fake[0])\n",
    "        g_hist.append(g_loss)\n",
    "        \n",
    "        if iteration % sample_interval == 0:\n",
    "            print('{} [D loss: {}, accuracy: {:.2f}] [D1 loss: {}][D2 loss: {}][G loss: {}]'.format(iteration, d_loss[0], 100 * d_loss[1],d_loss_real,d_loss_fake, g_loss))\n",
    "        \n",
    "            losses.append((d_loss[0], g_loss))\n",
    "            accuracies.append(d_loss[1])\n",
    "            \n",
    "\n",
    "    plot_history(c1_hist, c2_hist, g_hist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 20\n",
    "batch_size = 128\n",
    "sample_interval = 1#1000\n",
    "\n",
    "train(iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
